{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment (with discrete state space)"
      ],
      "metadata": {
        "id": "Lv9ByeHxEF3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V9vp-9CVedVh"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from scipy.stats import gamma, norm, expon\n",
        "\n",
        "class SteelProductionEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.action_space = gym.spaces.Discrete(2)\n",
        "        self.observation_space = gym.spaces.Tuple((\n",
        "            gym.spaces.Discrete(4),\n",
        "            gym.spaces.Discrete(5),\n",
        "            gym.spaces.Discrete(2),\n",
        "        ))\n",
        "        # First workstation\n",
        "        self.n = 20\n",
        "        self.h1 = 800\n",
        "        self.h2 = 200\n",
        "        self.ci = 50\n",
        "        self.cf = 3000\n",
        "        self.cu = 100\n",
        "        self.cr = 230\n",
        "        self.cs = 0.01\n",
        "\n",
        "        # Hammer degradation\n",
        "        self.alpha_T1 = 1\n",
        "        self.beta_T1 = 0.1\n",
        "        self.alpha_T2 = 0.5\n",
        "        self.beta_T2 = 0.12\n",
        "        self.hammer_degradation = np.zeros(self.n)\n",
        "        self.total_replaced_hammers = 0\n",
        "        self.last_num_defective_hammers = 0\n",
        "\n",
        "        # Production\n",
        "        self.c1 = 1.0\n",
        "        self.c2 = 0.8\n",
        "        self.current_Pt = self.c1 * max(0, norm.rvs(loc=1.0, scale=0.1)) * 20\n",
        "        self.production_pace = 'T1'\n",
        "\n",
        "        # Maintenance parameters\n",
        "        self.Tp = 4\n",
        "        self.Tu = 1\n",
        "        self.Ta_mean = 10\n",
        "\n",
        "        self.last_pm_time = 0\n",
        "        self.last_cm_time = 0\n",
        "        self.cumulative_cm = 0\n",
        "        self.first_ws_stopped = False\n",
        "        self.second_ws_stopped = False\n",
        "\n",
        "        # Second workstation\n",
        "        self.demand_mean = 15\n",
        "        self.demand_std = 1\n",
        "        self.failure_rate = 1/336\n",
        "        self.cm_duration_mean_ws2 = 3\n",
        "        self.pm_duration_ws2 = 2\n",
        "        self.WS2_age = 0\n",
        "        self.lifetime = expon.rvs(scale=1/self.failure_rate)\n",
        "        self.is_down = False\n",
        "        self.current_rul = self.lifetime\n",
        "        self.current_dt = norm.rvs(loc=self.demand_mean, scale=self.demand_std)\n",
        "        self.unmet_demand_during_m = 0\n",
        "\n",
        "        # Buffer\n",
        "        self.buffer_capacity = 1000\n",
        "        self.current_bt = 0\n",
        "\n",
        "        # State and action spaces\n",
        "        self.observation_space = gym.spaces.Tuple((\n",
        "            gym.spaces.Discrete(4),\n",
        "            gym.spaces.Discrete(5),\n",
        "            gym.spaces.Discrete(2)\n",
        "        ))\n",
        "        self.action_space = gym.spaces.Discrete(2)\n",
        "\n",
        "        self.current_state_Pt, self.current_state_bt, self.current_state_dt = self.get_state(\n",
        "            self.current_Pt, self.current_bt, self.current_dt\n",
        "        )\n",
        "        self.time = 0\n",
        "\n",
        "    def get_production_rate(self):\n",
        "        rho_t = max(0, norm.rvs(loc=1.0, scale=0.1))\n",
        "        hammer_contributions = np.zeros(self.n)\n",
        "        for i in range(self.n):\n",
        "            if self.hammer_degradation[i] < 0.2:\n",
        "                hammer_contributions[i] = 1.0\n",
        "            elif self.hammer_degradation[i] < 0.3:\n",
        "                hammer_contributions[i] = 0.8\n",
        "            elif self.hammer_degradation[i] < 0.4:\n",
        "                hammer_contributions[i] = 0.6\n",
        "            else:\n",
        "                hammer_contributions[i] = 0.0\n",
        "        if self.production_pace == 'T1':\n",
        "            Pt = self.c1 * rho_t * np.sum(hammer_contributions)\n",
        "        else:\n",
        "            Pt = self.c2 * rho_t * np.sum(hammer_contributions)\n",
        "        return Pt\n",
        "\n",
        "    def get_demand(self):\n",
        "        if self.second_ws_stopped:\n",
        "            return 0\n",
        "        else:\n",
        "            return norm.rvs(loc=self.demand_mean, scale=self.demand_std)\n",
        "\n",
        "    def get_rul(self):\n",
        "        rul = self.lifetime - self.WS2_age\n",
        "        return rul\n",
        "\n",
        "    def get_buffer_level(self, Pt, bt, dt):\n",
        "        new_bt = bt + Pt - dt\n",
        "        return max(0, min(new_bt, self.buffer_capacity))\n",
        "\n",
        "    def get_unmet_demand(self, Pt, bt, dt):\n",
        "        if dt > Pt + bt:\n",
        "            return dt - Pt - bt\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def get_production_pace(self, bt):\n",
        "        if bt >= self.h1:\n",
        "            self.production_pace = 'T2'\n",
        "        elif bt <= self.h2:\n",
        "            self.production_pace = 'T1'\n",
        "        return self.production_pace\n",
        "\n",
        "    def get_state(self, Pt, bt, dt):\n",
        "        if Pt >= 0.75 * self.c1 * self.n:\n",
        "            Pt_state = 0\n",
        "        elif Pt >= 0.5 * self.c1 * self.n:\n",
        "            Pt_state = 1\n",
        "        elif Pt >= 0.25 * self.c1 * self.n:\n",
        "            Pt_state = 2\n",
        "        else:\n",
        "            Pt_state = 3\n",
        "\n",
        "        if bt >= 800:\n",
        "            bt_state = 4\n",
        "        elif bt >= 600:\n",
        "            bt_state = 3\n",
        "        elif bt >= 400:\n",
        "            bt_state = 2\n",
        "        elif bt >= 200:\n",
        "            bt_state = 1\n",
        "        else:\n",
        "            bt_state = 0\n",
        "\n",
        "        if dt > 0:\n",
        "            dt_state = 1\n",
        "        else:\n",
        "            dt_state = 0\n",
        "\n",
        "        return Pt_state, bt_state, dt_state\n",
        "\n",
        "    def update_hammer_degradation(self):\n",
        "        if self.production_pace == 'T1':\n",
        "            alpha = self.alpha_T1\n",
        "            beta = self.beta_T1\n",
        "        else:\n",
        "            alpha = self.alpha_T2\n",
        "            beta = self.beta_T2\n",
        "\n",
        "        for i in range(self.n):\n",
        "            degradation_increment = gamma.rvs(a=alpha, scale=beta)\n",
        "            self.hammer_degradation[i] += degradation_increment\n",
        "            if self.hammer_degradation[i] > 1.0:\n",
        "                self.hammer_degradation[i] = 1.0\n",
        "\n",
        "    def pm_perform_ws1_action1(self):\n",
        "        self.last_num_defective_hammers = np.sum(self.hammer_degradation >= 0.2)\n",
        "        if self.last_num_defective_hammers <= 3:\n",
        "            replacement_cost = self.cr * 3\n",
        "        else:\n",
        "            replacement_cost = self.cr * self.last_num_defective_hammers\n",
        "        maintenance_duration = self.Tp + self.last_num_defective_hammers * self.Tu\n",
        "        self.hammer_degradation[self.hammer_degradation >= 0.2] = 0.0\n",
        "        self.last_pm_time = self.time\n",
        "        return maintenance_duration, replacement_cost\n",
        "\n",
        "    def pm_perform_ws1(self):\n",
        "        self.last_num_defective_hammers = np.sum(self.hammer_degradation >= 0.2)\n",
        "        replacement_cost = self.cr * self.last_num_defective_hammers\n",
        "        maintenance_duration = self.Tp + self.last_num_defective_hammers * self.Tu\n",
        "        self.hammer_degradation[self.hammer_degradation >= 0.2] = 0.0\n",
        "        self.last_pm_time = self.time\n",
        "        return maintenance_duration, replacement_cost\n",
        "\n",
        "    def cm_perform_ws1(self):\n",
        "        self.last_num_defective_hammers = np.sum(self.hammer_degradation >= 0.2)\n",
        "        replacement_cost = self.cr * self.last_num_defective_hammers\n",
        "        maintenance_duration = self.Tp + self.last_num_defective_hammers * self.Tu + expon.rvs(scale=self.Ta_mean)\n",
        "        self.hammer_degradation[self.hammer_degradation >= 0.2] = 0.0\n",
        "        self.last_cm_time = self.time\n",
        "        self.cumulative_cm += 1\n",
        "        return maintenance_duration, replacement_cost\n",
        "\n",
        "    def cm_perform_ws2(self):\n",
        "        cm_duration = expon.rvs(scale=self.cm_duration_mean_ws2)\n",
        "        self.lifetime = expon.rvs(scale=1/self.failure_rate)\n",
        "        self.WS2_age = 0\n",
        "        self.current_rul = self.lifetime\n",
        "        return cm_duration\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.hammer_degradation = np.zeros(self.n)\n",
        "        self.production_pace = 'T1'\n",
        "        self.time = 0\n",
        "        self.last_pm_time = 0\n",
        "        self.last_cm_time = 0\n",
        "        self.total_replaced_hammers = 0\n",
        "        self.unmet_demand_during_m = 0\n",
        "        self.current_Pt = self.c1 * max(0, norm.rvs(loc=1.0, scale=0.1)) * 20\n",
        "        self.current_dt = norm.rvs(loc=self.demand_mean, scale=self.demand_std)\n",
        "        self.lifetime = expon.rvs(scale=1/self.failure_rate)\n",
        "        self.current_rul = self.lifetime\n",
        "        self.current_state_Pt, self.current_state_bt, self.current_state_dt = self.get_state(\n",
        "            self.current_Pt, self.current_bt, self.current_dt\n",
        "        )\n",
        "\n",
        "        return (self.current_state_Pt, self.current_state_bt, self.current_state_dt), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        self.first_ws_stopped = False\n",
        "        self.second_ws_stopped = False\n",
        "        time_before_m = self.time\n",
        "        Pt_initial = self.current_Pt\n",
        "        dt_initial = self.current_dt\n",
        "        bt_initial = self.current_bt\n",
        "        lifetime_initial = self.lifetime\n",
        "        rul_initial = self.current_rul\n",
        "        Pt_state_initial = self.current_state_Pt\n",
        "        bt_state_initial = self.current_state_bt\n",
        "        dt_state_initial = self.current_state_dt\n",
        "        age_before_m = self.WS2_age\n",
        "        hammer_degradation_initial = self.hammer_degradation.copy()\n",
        "        production_pace_initial = self.production_pace\n",
        "\n",
        "        ws2_maintenance_time = 0\n",
        "        ws1_maintenance_time = 0\n",
        "\n",
        "        if self.current_state_Pt == 3:\n",
        "            cm_duration_ws1, replacement_cost = self.cm_perform_ws1()\n",
        "            reward -= self.cf + self.ci + replacement_cost\n",
        "            self.current_Pt = 0\n",
        "            cm_duration_ws2 = 0\n",
        "\n",
        "            if self.current_bt == 0:\n",
        "                self.current_dt = 0\n",
        "                self.second_ws_stopped = True\n",
        "                if self.current_rul <= 0:\n",
        "                    cm_duration_ws2 = self.cm_perform_ws2()\n",
        "\n",
        "            self.first_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "        elif self.time > 0 and self.current_bt <= 0:\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            self.current_Pt = 0\n",
        "            cm_duration_ws2 = 0\n",
        "\n",
        "            if self.current_rul <= 0:\n",
        "                cm_duration_ws2 = self.cm_perform_ws2()\n",
        "\n",
        "            self.current_dt = 0\n",
        "            self.second_ws_stopped = True\n",
        "            self.first_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "        elif self.time > 0 and self.current_bt > self.buffer_capacity:\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            self.current_Pt = 0\n",
        "            cm_duration_ws2 = 0\n",
        "\n",
        "            self.first_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "\n",
        "        elif self.current_rul <= 0:\n",
        "            cm_duration = self.cm_perform_ws2()\n",
        "            ws2_maintenance_time = cm_duration\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            ws1_maintenance_time = maintenance_duration\n",
        "\n",
        "            self.current_dt = 0\n",
        "            self.current_Pt = 0\n",
        "            self.first_ws_stopped = True\n",
        "            self.second_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "\n",
        "        elif self.time > 0 and int(self.time) % 24 == 0:\n",
        "            if self.current_rul <= 0:\n",
        "                cm_duration = self.cm_perform_ws2()\n",
        "                ws2_maintenance_time = cm_duration\n",
        "            else:\n",
        "                ws2_maintenance_time = self.pm_duration_ws2\n",
        "\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            ws1_maintenance_time = maintenance_duration\n",
        "\n",
        "            self.current_dt = 0\n",
        "            self.current_Pt = 0\n",
        "            self.first_ws_stopped = True\n",
        "            self.second_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "\n",
        "        elif action == 1:\n",
        "            last_num_defective_hammers = np.sum(self.hammer_degradation >= 0.2)\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1_action1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            self.current_Pt = 0\n",
        "            cm_duration_ws2 = 0\n",
        "\n",
        "            if self.time > 0 and self.current_bt <= 0:\n",
        "                self.current_dt = 0\n",
        "                self.second_ws_stopped = True\n",
        "                if self.current_rul <= 0:\n",
        "                    cm_duration_ws2 = self.cm_perform_ws2()\n",
        "\n",
        "            self.first_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "        hammer_degradation_after_maintenance = self.hammer_degradation.copy()\n",
        "        time_after_m = self.time\n",
        "        age_after_m = self.WS2_age\n",
        "\n",
        "        self.update_hammer_degradation()\n",
        "        self.time += 1\n",
        "        self.WS2_age += 0 if self.second_ws_stopped else 1\n",
        "        self.current_Pt = self.get_production_rate()\n",
        "        self.current_dt = self.get_demand()\n",
        "        self.current_bt = self.get_buffer_level(self.current_Pt, self.current_bt, self.current_dt)\n",
        "        self.current_rul = self.get_rul()\n",
        "        self.current_state_Pt, self.current_state_bt, self.current_state_dt = self.get_state(\n",
        "            self.current_Pt, self.current_bt, self.current_dt\n",
        "        )\n",
        "\n",
        "        unmet_demand = self.get_unmet_demand(self.current_Pt, self.current_bt, self.current_dt)\n",
        "        self.production_pace = self.get_production_pace(self.current_bt)\n",
        "        reward -= self.cu * unmet_demand + self.cs * self.current_bt\n",
        "\n",
        "        if self.time >= 1000:\n",
        "            terminated = True\n",
        "\n",
        "        '''print(f\"\"\"\n",
        "*** Inspection and Maintenance Times:\n",
        "    Time Before Maintenance(valid if it's in a maintenance state): {time_before_m}\n",
        "    Time After Maintenance(valid if it's in a maintenance state): {time_after_m if 'time_after_m' in locals() else 'N/A'}\n",
        "    Inspection Time At the End: {self.time}\n",
        "    Action: {action}\n",
        "    Reward: {reward}\n",
        "\n",
        "\n",
        "*** Initial Values:\n",
        "    Production Rate (Pt): {Pt_initial}\n",
        "    Demand (dt): {dt_initial}\n",
        "    Buffer level (bt): {bt_initial}\n",
        "    State -> Pt_state: {Pt_state_initial}, bt_state: {bt_state_initial}\n",
        "    Hammer Degradation (Initial): {hammer_degradation_initial}\n",
        "    production pace: {production_pace_initial}\n",
        "\"\"\")\n",
        "\n",
        "        if self.first_ws_stopped:\n",
        "            print(f\"\"\"\n",
        "*** During Maintenance:\n",
        "    Production Rate (Pt): {Pt_during_m}\n",
        "    Demand (dt): {dt_during_m}\n",
        "    Buffer level (bt): {bt_during_m}\n",
        "\"\"\")\n",
        "\n",
        "        print(f\"\"\"\n",
        "*** After 1 step:\n",
        "    Production Rate (Pt): {self.current_Pt}\n",
        "    Demand (dt): {self.current_dt}\n",
        "    Buffer level (bt): {self.current_bt}\n",
        "    State -> Pt_state: {self.current_state_Pt}, bt_state: {self.current_state_bt}, dt_state: {self.current_state_dt}\n",
        "    Total Replaced Hammers: {self.last_num_defective_hammers}\n",
        "    Hammer Degradation (After Maintenance): {hammer_degradation_after_maintenance if 'hammer_degradation_after_maintenance' in locals() else hammer_degradation_initial}\n",
        "    Hammer Degradation (After 1 step): {self.hammer_degradation}\n",
        "    production pace: {self.production_pace}\n",
        "    Unmet Demand: {unmet_demand}\n",
        "\"\"\")\n",
        "\n",
        "        print(f\"\"\"\n",
        "*** next state: (Pt, bt, dt):{self.current_state_Pt, self.current_state_bt, self.current_state_dt}\n",
        "\n",
        "\"\"\")\n",
        "        print(\"-\" * 150)'''\n",
        "\n",
        "        return (\n",
        "            (self.current_state_Pt, self.current_state_bt, self.current_state_dt),\n",
        "            reward,\n",
        "            terminated,\n",
        "            truncated,\n",
        "            {}\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyMpMSTijVsP"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3 gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDQN"
      ],
      "metadata": {
        "id": "mIPObR88VRW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7LSmGEuk8-q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "import gymnasium as gym\n",
        "\n",
        "class TupleToArrayWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=10, shape=(3,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "\n",
        "class DoubleDQN(DQN):\n",
        "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
        "        self.policy.set_training_mode(True)\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "\n",
        "        losses = []\n",
        "        for _ in range(gradient_steps):\n",
        "            replay_data = self.replay_buffer.sample(\n",
        "                batch_size, env=self._vec_normalize_env\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                next_q_online = self.policy.q_net(replay_data.next_observations)\n",
        "                next_actions = next_q_online.argmax(dim=1, keepdim=True)\n",
        "\n",
        "                next_q_target_all = self.policy.q_net_target(replay_data.next_observations)\n",
        "                next_q = torch.gather(next_q_target_all, 1, next_actions).reshape(-1, 1)\n",
        "\n",
        "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q\n",
        "\n",
        "            current_q_values = self.policy.q_net(replay_data.observations)\n",
        "            current_q_values = torch.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
        "\n",
        "            loss = torch.nn.functional.smooth_l1_loss(current_q_values, target_q_values)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            self.policy.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "            self.policy.optimizer.step()\n",
        "\n",
        "        self._n_updates += gradient_steps\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        self.logger.record(\"train/loss\", float(np.mean(losses)))\n",
        "\n",
        "\n",
        "class ExponentialDecayCallback(BaseCallback):\n",
        "    def __init__(self, decay_rate=0.999, min_epsilon=0.05, verbose=0):\n",
        "        super(ExponentialDecayCallback, self).__init__(verbose)\n",
        "        self.decay_rate = decay_rate\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.model.exploration_rate > self.min_epsilon:\n",
        "            self.model.exploration_rate = max(\n",
        "                self.model.exploration_rate * self.decay_rate,\n",
        "                self.min_epsilon\n",
        "            )\n",
        "        return True\n",
        "\n",
        "class RewardTrackerCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(RewardTrackerCallback, self).__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "        self.current_episode_reward = 0\n",
        "        self.episode_lengths = []\n",
        "        self.current_episode_length = 0\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        reward = self.locals['rewards'][0]\n",
        "        done = self.locals['dones'][0]\n",
        "\n",
        "        self.current_episode_reward += reward\n",
        "        self.current_episode_length += 1\n",
        "\n",
        "        if done:\n",
        "            self.episode_rewards.append(self.current_episode_reward)\n",
        "            self.episode_lengths.append(self.current_episode_length)\n",
        "            self.current_episode_reward = 0\n",
        "            self.current_episode_length = 0\n",
        "\n",
        "        return True\n",
        "\n",
        "    def plot_rewards(self):\n",
        "        \"\"\"Plot the reward convergence\"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        window_size = max(1, len(self.episode_rewards) // 50)\n",
        "        moving_avg = np.convolve(\n",
        "            self.episode_rewards,\n",
        "            np.ones(window_size)/window_size,\n",
        "            mode='valid'\n",
        "        )\n",
        "\n",
        "        plt.plot(self.episode_rewards, alpha=0.3, label='Episode Reward')\n",
        "        plt.plot(\n",
        "            range(window_size-1, window_size-1 + len(moving_avg)),\n",
        "            moving_avg,\n",
        "            'r-',\n",
        "            linewidth=2,\n",
        "            label=f'Moving Avg ({window_size} episodes)'\n",
        "        )\n",
        "\n",
        "        plt.title('Reward Convergence During Training')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Total Reward')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('reward_convergence.png')\n",
        "        plt.show()\n",
        "\n",
        "hyperparams = {\n",
        "    \"learning_rate\": 0.003,\n",
        "    \"buffer_size\": 1000,\n",
        "    \"batch_size\": 64,\n",
        "    \"tau\": 1.0,\n",
        "    \"gamma\": 0.99,\n",
        "    \"train_freq\": (1, \"step\"),\n",
        "    \"target_update_interval\": 10,\n",
        "    \"exploration_final_eps\": 0.05,\n",
        "    \"exploration_initial_eps\": 1.0,\n",
        "    \"learning_starts\": 64,\n",
        "    \"policy_kwargs\": {\n",
        "         \"net_arch\": [32, 32],\n",
        "        \"activation_fn\": nn.ReLU\n",
        "    },\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "env = make_vec_env(\n",
        "    lambda: TupleToArrayWrapper(SteelProductionEnv()),\n",
        "    n_envs=1,\n",
        "    vec_env_cls=DummyVecEnv\n",
        ")\n",
        "\n",
        "model = DoubleDQN(\"MlpPolicy\", env, **hyperparams)\n",
        "\n",
        "decay_callback = ExponentialDecayCallback()\n",
        "reward_tracker = RewardTrackerCallback()\n",
        "callback_list = CallbackList([decay_callback, reward_tracker])\n",
        "\n",
        "total_steps = 1000 * 1000\n",
        "model.learn(\n",
        "    total_timesteps=total_steps,\n",
        "    callback=callback_list,\n",
        "    log_interval=100\n",
        ")\n",
        "\n",
        "model.save(\"DDQN\")\n",
        "\n",
        "reward_tracker.plot_rewards()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKjnJtCw5wzQ"
      },
      "outputs": [],
      "source": [
        "# discrete DDQN\n",
        "\n",
        "env_discrete = make_vec_env(\n",
        "    lambda: SteelProductionEnv(),\n",
        "    n_envs=1,\n",
        "    vec_env_cls=DummyVecEnv\n",
        ")\n",
        "model_discrete = DoubleDQN(\"MlpPolicy\", env, **hyperparams)\n",
        "model_discrete.load(\"DDQN\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the trained DDQN agent"
      ],
      "metadata": {
        "id": "rcB8Z1xeOqlN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYDdjJyeZkfJ"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------\n",
        "# Evaluation of the trained agents\n",
        "#    with greedy action selection\n",
        "# -----------------------------------------------\n",
        "\n",
        "n = 20\n",
        "c1 = 1\n",
        "\n",
        "env_discrete = SteelProductionEnv()\n",
        "\n",
        "eval_episodes = 500\n",
        "eval_epsilon  = 0\n",
        "eval_rewards_ddqn_discrete = []\n",
        "\n",
        "\n",
        "#Discrete DDQN evaluation\n",
        "for _ in range(eval_episodes):\n",
        "    obs, _ = env_discrete.reset()\n",
        "    ep_reward_discrete = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model_discrete.predict(obs, deterministic=True)\n",
        "        obs, reward, done, truncated, info = env_discrete.step(action)\n",
        "        ep_reward_discrete += reward\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    eval_rewards_ddqn_discrete.append(ep_reward_discrete)\n",
        "\n",
        "\n",
        "print(\"DDQN (discrete) results:\")\n",
        "print(f\"Îµ = {eval_epsilon}\")\n",
        "print(f\"Average reward from the discrete DDQN trained agent (thesis) during {eval_episodes} episodes: {np.mean(eval_rewards_ddqn_discrete):.2f}\")\n",
        "print(f\"SD reward from the DDQN trained agent (thesis) during {eval_episodes} episodes: {np.std(eval_rewards_ddqn_discrete):.2f}\")\n",
        "print(f\"Max episode reward: {np.max(eval_rewards_ddqn_discrete):.2f}\")\n",
        "print(f\"Min episode reward: {np.min(eval_rewards_ddqn_discrete):.2f}\")\n"
      ]
    }
  ]
}