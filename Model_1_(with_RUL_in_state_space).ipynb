{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 Environment (without RUL in state)"
      ],
      "metadata": {
        "id": "Lv9ByeHxEF3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import gamma, norm, expon\n",
        "from collections import defaultdict\n",
        "\n",
        "class SteelProductionLine:\n",
        "    def __init__(self):\n",
        "\n",
        "          # First workstation\n",
        "          self.n = 20\n",
        "          self.h1 = 800\n",
        "          self.h2 = 200\n",
        "          self.ci = 50\n",
        "          self.cf = 3000\n",
        "          self.cu = 100\n",
        "          self.cr = 230\n",
        "          self.cs = 0.01\n",
        "\n",
        "          # Hammer degradation\n",
        "          self.alpha_T1 = 1\n",
        "          self.beta_T1 = 0.1\n",
        "          self.alpha_T2 = 0.5\n",
        "          self.beta_T2 = 0.12\n",
        "          self.hammer_degradation = np.zeros(self.n)\n",
        "          self.total_replaced_hammers = 0\n",
        "          self.last_num_defective_hammers = 0\n",
        "\n",
        "          # Production\n",
        "          self.c1 = 1.0\n",
        "          self.c2 = 0.8\n",
        "          self.current_Pt = self.c1 * max(0, norm.rvs(loc=1.0, scale=0.1)) * 20\n",
        "          self.production_pace = 'T1'\n",
        "\n",
        "          # Maintenance parameters\n",
        "          self.Tp = 4\n",
        "          self.Tu = 1\n",
        "          self.Ta_mean = 10\n",
        "\n",
        "          self.last_pm_time = 0\n",
        "          self.last_cm_time = 0\n",
        "          self.cumulative_cm = 0\n",
        "          self.first_ws_stopped = False\n",
        "          self.second_ws_stopped = False\n",
        "\n",
        "          # Second workstation\n",
        "          self.demand_mean = 15\n",
        "          self.demand_std = 1\n",
        "          self.failure_rate = 1/336\n",
        "          self.cm_duration_mean_ws2 = 3\n",
        "          self.pm_duration_ws2 = 2\n",
        "          self.WS2_age = 0\n",
        "          self.lifetime = expon.rvs(scale=1/self.failure_rate)\n",
        "          self.is_down = False\n",
        "          self.current_rul = self.lifetime\n",
        "          self.current_dt = norm.rvs(loc=self.demand_mean, scale=self.demand_std)\n",
        "          self.unmet_demand_during_m = 0\n",
        "\n",
        "          # Buffer\n",
        "          self.buffer_capacity = 1000\n",
        "          self.current_bt = 0\n",
        "\n",
        "          # Initial state\n",
        "          self.current_state_Pt, self.current_state_bt, self.current_state_dt = self.get_state(self.current_Pt, self.current_bt, self.current_dt)\n",
        "          self.time = 0\n",
        "\n",
        "\n",
        "    def get_production_rate(self):\n",
        "        rho_t = max(0, norm.rvs(loc=1.0, scale=0.1))\n",
        "        hammer_contributions = np.zeros(self.n)\n",
        "        for i in range(self.n):\n",
        "            if self.hammer_degradation[i] < 0.2:\n",
        "                hammer_contributions[i] = 1.0\n",
        "            elif self.hammer_degradation[i] < 0.3:\n",
        "                hammer_contributions[i] = 0.8\n",
        "            elif self.hammer_degradation[i] < 0.4:\n",
        "                hammer_contributions[i] = 0.6\n",
        "            else:\n",
        "                hammer_contributions[i] = 0.0\n",
        "        if self.production_pace == 'T1':\n",
        "            Pt = self.c1 * rho_t * np.sum(hammer_contributions)\n",
        "        else:\n",
        "            Pt = self.c2 * rho_t * np.sum(hammer_contributions)\n",
        "        return Pt\n",
        "\n",
        "\n",
        "    def get_demand(self):\n",
        "        #Generating demand for the second workstation\n",
        "        if self.second_ws_stopped:\n",
        "            return 0\n",
        "        else:\n",
        "            return norm.rvs(loc=self.demand_mean, scale=self.demand_std)\n",
        "\n",
        "    def get_rul(self):\n",
        "        rul = self.lifetime - self.WS2_age\n",
        "        return rul\n",
        "\n",
        "    def get_buffer_level(self, Pt, bt, dt):\n",
        "        new_bt = bt + Pt - dt\n",
        "        return max(0, min(new_bt, self.buffer_capacity))\n",
        "\n",
        "    def get_unmet_demand(self, Pt, bt, dt):\n",
        "        if dt > Pt + bt:\n",
        "          return dt - Pt - bt\n",
        "        else:\n",
        "          return 0\n",
        "\n",
        "    def get_production_pace(self, bt):\n",
        "        if bt >= self.h1:\n",
        "            self.production_pace = 'T2'\n",
        "        elif bt <= self.h2:\n",
        "            self.production_pace = 'T1'\n",
        "        return self.production_pace\n",
        "\n",
        "    def get_state(self, Pt, bt, dt):\n",
        "          # State aggregation for pt\n",
        "          if Pt >= 0.75 * self.c1 * self.n:\n",
        "              Pt_state = 0\n",
        "          elif Pt >= 0.5 * self.c1 * self.n:\n",
        "              Pt_state = 1\n",
        "          elif Pt >= 0.25 * self.c1 * self.n:\n",
        "              Pt_state = 2\n",
        "          else:\n",
        "              Pt_state = 3\n",
        "\n",
        "          # State aggregation for bt\n",
        "          if bt >= 800:\n",
        "              bt_state = 4\n",
        "          elif bt >= 600:\n",
        "              bt_state = 3\n",
        "          elif bt >= 400:\n",
        "              bt_state = 2\n",
        "          elif bt >= 200:\n",
        "              bt_state = 1\n",
        "          else:\n",
        "              bt_state = 0\n",
        "\n",
        "          # State aggregation for dt (demand)\n",
        "          if dt > 0:\n",
        "              dt_state = 1\n",
        "          else:\n",
        "              dt_state = 0\n",
        "\n",
        "\n",
        "          return Pt_state, bt_state, dt_state\n",
        "\n",
        "    def update_hammer_degradation(self):\n",
        "      if self.production_pace == 'T1':\n",
        "          alpha = self.alpha_T1\n",
        "          beta = self.beta_T1\n",
        "      else:\n",
        "          alpha = self.alpha_T2\n",
        "          beta = self.beta_T2\n",
        "\n",
        "      for i in range(self.n):\n",
        "          degradation_increment = gamma.rvs(a=alpha, scale=beta)\n",
        "          self.hammer_degradation[i] += degradation_increment\n",
        "          if self.hammer_degradation[i] > 1.0:\n",
        "              self.hammer_degradation[i] = 1.0\n",
        "\n",
        "    def pm_perform_ws1_action1(self):\n",
        "        self.last_num_defective_hammers = np.sum(self.hammer_degradation >= 0.2)\n",
        "        if self.last_num_defective_hammers <= 3:\n",
        "            replacement_cost = self.cr * 3\n",
        "        else:\n",
        "            replacement_cost = self.cr * self.last_num_defective_hammers\n",
        "        maintenance_duration = self.Tp + self.last_num_defective_hammers * self.Tu\n",
        "        self.hammer_degradation[self.hammer_degradation >= 0.2] = 0.0\n",
        "        self.last_pm_time = self.time\n",
        "        return maintenance_duration, replacement_cost\n",
        "\n",
        "\n",
        "    def pm_perform_ws1(self):\n",
        "        self.last_num_defective_hammers = np.sum(self.hammer_degradation >= 0.2)\n",
        "        replacement_cost = self.cr * self.last_num_defective_hammers\n",
        "        maintenance_duration = self.Tp + self.last_num_defective_hammers * self.Tu\n",
        "        self.hammer_degradation[self.hammer_degradation >= 0.2] = 0.0\n",
        "        self.last_pm_time = self.time\n",
        "        return maintenance_duration, replacement_cost\n",
        "\n",
        "\n",
        "    def cm_perform_ws1(self):\n",
        "        self.last_num_defective_hammers = np.sum(self.hammer_degradation >= 0.2)\n",
        "        replacement_cost = self.cr * self.last_num_defective_hammers\n",
        "        maintenance_duration = self.Tp + self.last_num_defective_hammers * self.Tu + expon.rvs(scale=self.Ta_mean)\n",
        "        self.hammer_degradation[self.hammer_degradation >= 0.2] = 0.0\n",
        "        self.last_cm_time = self.time\n",
        "        self.cumulative_cm += 1\n",
        "        return maintenance_duration, replacement_cost\n",
        "\n",
        "\n",
        "    def cm_perform_ws2(self):\n",
        "        cm_duration = expon.rvs(scale=self.cm_duration_mean_ws2)\n",
        "        self.lifetime = expon.rvs(scale=1/self.failure_rate)\n",
        "        self.WS2_age = 0\n",
        "        self.current_rul = self.lifetime\n",
        "        return cm_duration\n",
        "\n",
        "    def reset(self):\n",
        "        self.hammer_degradation = np.zeros(self.n)\n",
        "        self.production_pace = 'T1'\n",
        "        self.time = 0\n",
        "        self.last_pm_time = 0\n",
        "        self.last_cm_time = 0\n",
        "        self.total_replaced_hammers = 0\n",
        "        self.unmet_demand_during_m = 0\n",
        "        self.current_Pt = self.c1 * max(0, norm.rvs(loc=1.0, scale=0.1)) * 20\n",
        "        self.current_dt = norm.rvs(loc=self.demand_mean, scale=self.demand_std)\n",
        "        self.lifetime = expon.rvs(scale=1/self.failure_rate)\n",
        "        self.current_rul = self.lifetime\n",
        "        self.current_state_Pt, self.current_state_bt, self.current_state_dt = self.get_state(self.current_Pt, self.current_bt, self.current_dt)\n",
        "\n",
        "        return self.current_state_Pt, self.current_state_bt, self.current_state_dt\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        done = False\n",
        "\n",
        "        self.first_ws_stopped = False\n",
        "        self.second_ws_stopped = False\n",
        "        time_before_m = self.time\n",
        "        Pt_initial = self.current_Pt\n",
        "        dt_initial = self.current_dt\n",
        "        bt_initial = self.current_bt\n",
        "        lifetime_initial = self.lifetime\n",
        "        rul_initial = self.current_rul\n",
        "        Pt_state_initial = self.current_state_Pt\n",
        "        bt_state_initial = self.current_state_bt\n",
        "        dt_state_initial = self.current_state_dt\n",
        "        age_before_m = self.WS2_age\n",
        "        hammer_degradation_initial = self.hammer_degradation.copy()\n",
        "        production_pace_initial = self.production_pace\n",
        "\n",
        "        ws2_maintenance_time = 0\n",
        "        ws1_maintenance_time = 0\n",
        "\n",
        "\n",
        "        if self.current_state_Pt == 3:\n",
        "            cm_duration_ws1, replacement_cost = self.cm_perform_ws1()\n",
        "            reward -= self.cf + self.ci + replacement_cost\n",
        "            self.current_Pt = 0\n",
        "            cm_duration_ws2 = 0\n",
        "            if self.current_bt == 0:\n",
        "                  self.current_dt = 0\n",
        "                  self.second_ws_stopped = True\n",
        "                  if self.current_rul <= 0:\n",
        "                    cm_duration_ws2 = self.cm_perform_ws2()\n",
        "            self.WS2_age += max(cm_duration_ws1, cm_duration_ws2)\n",
        "\n",
        "\n",
        "            self.first_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "        elif self.time > 0 and self.current_bt <= 0:\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            self.current_Pt = 0\n",
        "            cm_duration_ws2 = 0\n",
        "            if self.current_rul <= 0:\n",
        "                cm_duration_ws2 = self.cm_perform_ws2()\n",
        "\n",
        "            self.WS2_age += max(maintenance_duration, cm_duration_ws2)\n",
        "            self.current_dt = 0\n",
        "            self.second_ws_stopped = True\n",
        "            self.first_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "        elif self.time > 0 and self.current_bt > self.buffer_capacity:\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            self.current_Pt = 0\n",
        "            cm_duration_ws2 = 0\n",
        "            self.WS2_age += max(cm_duration_ws2, maintenance_duration)\n",
        "\n",
        "            self.first_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "        elif self.current_rul <= 0:\n",
        "            cm_duration = self.cm_perform_ws2()\n",
        "            ws2_maintenance_time = cm_duration\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            ws1_maintenance_time = maintenance_duration\n",
        "\n",
        "            self.current_dt = 0\n",
        "            self.current_Pt = 0\n",
        "            max_maintenance_time = max(ws1_maintenance_time, ws2_maintenance_time)\n",
        "            self.WS2_age += max_maintenance_time\n",
        "\n",
        "            self.first_ws_stopped = True\n",
        "            self.second_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "        elif self.time > 0 and int(self.time) % 24 == 0:\n",
        "            if self.current_rul <= 0:\n",
        "                cm_duration = self.cm_perform_ws2()\n",
        "                ws2_maintenance_time = cm_duration\n",
        "            else:\n",
        "\n",
        "                ws2_maintenance_time = self.pm_duration_ws2\n",
        "\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            ws1_maintenance_time = maintenance_duration\n",
        "\n",
        "            self.current_dt = 0\n",
        "            self.current_Pt = 0\n",
        "            max_maintenance_time = max(ws1_maintenance_time, ws2_maintenance_time)\n",
        "            self.WS2_age += max_maintenance_time\n",
        "\n",
        "            self.first_ws_stopped = True\n",
        "            self.second_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "        elif action == 1:\n",
        "            last_num_defective_hammers = np.sum(self.hammer_degradation >= 0.2)\n",
        "            maintenance_duration, replacement_cost = self.pm_perform_ws1_action1()\n",
        "            reward -= self.ci + replacement_cost\n",
        "            self.current_Pt = 0\n",
        "            cm_duration_ws2 = 0\n",
        "            if self.time > 0 and self.current_bt <= 0:\n",
        "                  self.current_dt = 0\n",
        "                  self.second_ws_stopped = True\n",
        "                  if self.current_rul <= 0:\n",
        "                      cm_duration_ws2 = self.cm_perform_ws2()\n",
        "            self.WS2_age += max(cm_duration_ws2, maintenance_duration)\n",
        "\n",
        "            self.first_ws_stopped = True\n",
        "            Pt_during_m = self.current_Pt\n",
        "            bt_during_m = self.current_bt\n",
        "            dt_during_m = self.current_dt\n",
        "\n",
        "\n",
        "        hammer_degradation_after_maintenance = self.hammer_degradation.copy()\n",
        "        time_after_m = self.time\n",
        "        age_after_m = self.WS2_age\n",
        "\n",
        "\n",
        "        self.update_hammer_degradation()\n",
        "        self.time += 1\n",
        "        self.WS2_age += 1\n",
        "        self.current_Pt = self.get_production_rate()\n",
        "        self.current_dt = self.get_demand()\n",
        "        self.current_bt = self.get_buffer_level(self.current_Pt, self.current_bt, self.current_dt)\n",
        "        self.current_rul = self.get_rul()\n",
        "        self.current_state_Pt, self.current_state_bt, self.current_state_dt = self.get_state(self.current_Pt, self.current_bt, self.current_dt)\n",
        "\n",
        "\n",
        "        unmet_demand = self.get_unmet_demand(self.current_Pt, self.current_bt, self.current_dt)\n",
        "        self.production_pace = self.get_production_pace(self.current_bt)\n",
        "        reward -= self.cu * unmet_demand + self.cs * self.current_bt\n",
        "\n",
        "        done = self.time >= 1000\n",
        "\n",
        "        '''print(f\"\"\"\n",
        "*** Inspection and Maintenance Times:\n",
        "    Time Before Maintenance(valid if it's in a maintenance state): {time_before_m}\n",
        "    Time After Maintenance(valid if it's in a maintenance state): {time_after_m}\n",
        "    Inspection Time At the End: {self.time}\n",
        "    Action: {action}\n",
        "    Reward: {reward}\n",
        "\n",
        "\n",
        "*** Initial Values:\n",
        "    Production Rate (Pt): {Pt_initial}\n",
        "    Demand (dt): {dt_initial}\n",
        "    Buffer level (bt): {bt_initial}\n",
        "    State -> Pt_state: {Pt_state_initial}, bt_state: {bt_state_initial}\n",
        "    Hammer Degradation (Initial): {hammer_degradation_initial}\n",
        "    production pace: {production_pace_initial}\n",
        "\"\"\")\n",
        "\n",
        "        if self.first_ws_stopped:\n",
        "              print(f\"\"\"\n",
        "*** During Maintenance:\n",
        "    Production Rate (Pt): {Pt_during_m}\n",
        "    Demand (dt): {dt_during_m}\n",
        "    Buffer level (bt): {bt_during_m}\n",
        "\"\"\")\n",
        "\n",
        "        print(f\"\"\"\n",
        "*** After 1 step:\n",
        "    Production Rate (Pt): {self.current_Pt}\n",
        "    Demand (dt): {self.current_dt}\n",
        "    Buffer level (bt): {self.current_bt}\n",
        "    State -> Pt_state: {self.current_state_Pt}, bt_state: {self.current_state_bt}, dt_state: {self.current_state_dt}\n",
        "    Total Replaced Hammers: {self.last_num_defective_hammers}\n",
        "    Hammer Degradation (After Maintenance): {hammer_degradation_after_maintenance}\n",
        "    Hammer Degradation (After 1 step): {self.hammer_degradation}\n",
        "    production pace: {self.production_pace}\n",
        "    Unmet Demand: {unmet_demand}\n",
        "\"\"\")\n",
        "\n",
        "        print(f\"\"\"\n",
        "*** next state: (Pt, bt, dt):{self.current_state_Pt, self.current_state_bt, self.current_state_dt}\n",
        "\n",
        "\"\"\")\n",
        "        print(\"-\" * 150)'''\n",
        "\n",
        "        return self.current_state_Pt, self.current_state_bt, self.current_state_dt, reward, done\n",
        "\n"
      ],
      "metadata": {
        "id": "Uh7VVp3Li7lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning for Model 1"
      ],
      "metadata": {
        "id": "AhzHtwrT9YUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 10000\n",
        "max_steps = 500\n",
        "learning_rate = 0.01\n",
        "discount_factor = 0.99\n",
        "epsilon = 0.7\n",
        "epsilon_decay = 0.9995\n",
        "min_epsilon = 0.01\n",
        "\n",
        "env = SteelProductionLine()\n",
        "\n",
        "num_pt_states = 4\n",
        "num_bt_states = 5\n",
        "num_dt_states = 2\n",
        "num_states = num_pt_states * num_bt_states * num_dt_states\n",
        "num_actions = 2\n",
        "\n",
        "\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "\n",
        "def state_to_index(state):\n",
        "    Pt_state, bt_state, dt_state = state\n",
        "    index = Pt_state * (num_bt_states * num_dt_states) + bt_state * (num_dt_states) + dt_state\n",
        "    return int(index)\n",
        "\n",
        "\n",
        "state_counts = defaultdict(int)\n",
        "\n",
        "\n",
        "reward_list = []\n",
        "q_value_changes = []\n",
        "\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state_idx = state_to_index(state)\n",
        "    total_reward = 0\n",
        "    episode_q_value_change = 0\n",
        "    num_step = 0\n",
        "    for step in range(max_steps):\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            action = np.argmax(Q[state_idx])\n",
        "\n",
        "        next_state_pt, next_state_bt, next_state_dt, reward, done = env.step(action)\n",
        "        next_state = (next_state_pt, next_state_bt, next_state_dt)\n",
        "        next_state_idx = state_to_index(next_state)\n",
        "\n",
        "        old_q_value = Q[state_idx, action]\n",
        "        Q[state_idx, action] += learning_rate * (\n",
        "            reward + discount_factor * np.max(Q[next_state_idx]) - old_q_value)\n",
        "\n",
        "        episode_q_value_change += abs(Q[state_idx, action] - old_q_value)\n",
        "        state_counts[state_idx] += 1\n",
        "\n",
        "        state = next_state\n",
        "        state_idx = next_state_idx\n",
        "        total_reward += reward\n",
        "        num_step += 1\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "    reward_list.append(total_reward)\n",
        "    q_value_changes.append(episode_q_value_change)\n",
        "\n",
        "\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon}, Steps: {num_step}\")\n",
        "\n",
        "\n",
        "print(\"Q-Table:\")\n",
        "print(Q)\n",
        "\n",
        "\n",
        "print(\"State distribution:\")\n",
        "for state_idx, count in state_counts.items():\n",
        "    print(f\"State {state_idx}: {count} occurrences\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NsZNSL0xZ6E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots and Q-table"
      ],
      "metadata": {
        "id": "EIwKFtgBEaaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot total reward over episodes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(reward_list)\n",
        "plt.title('Total Reward Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xri5T4CF0WVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving Average Reward Over Episodes\n",
        "\n",
        "window_size = 100\n",
        "moving_average = np.convolve(reward_list, np.ones(window_size), 'valid') / window_size\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(moving_average)\n",
        "plt.title('Moving Average Reward Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Moving Average Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ThIiFPuVI9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_pt_states = 4  # P=0,1,2,3\n",
        "num_bt_states = 5  # B=0,1,2,3,4\n",
        "num_dt_states = 2  # D=0,1\n",
        "\n",
        "table_data = []\n",
        "for bt in range(num_bt_states):\n",
        "    for dt in range(num_dt_states):\n",
        "        row_header = [f\"B={bt}\", f\"D={dt}\"]\n",
        "        action_values = []\n",
        "        for Pt in range(num_pt_states):\n",
        "            for action in [0, 1]:\n",
        "                state_idx = Pt * (num_bt_states * num_dt_states) + bt * num_dt_states + dt\n",
        "                action_values.append(Q[state_idx][action])\n",
        "        table_data.append(row_header + action_values)\n",
        "\n",
        "columns = ['B', 'D']\n",
        "for Pt in range(num_pt_states):\n",
        "    columns.extend([f'P={Pt} (A0)', f'P={Pt} (A1)'])\n",
        "\n",
        "df = pd.DataFrame(table_data, columns=columns)\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "sns.set(font_scale=0.8)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "heatmap = sns.heatmap(\n",
        "    df.set_index(['B', 'D']),\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=\"YlGnBu\",\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={'label': 'Q-Value'},\n",
        "    annot_kws={'size': 8}\n",
        ")\n",
        "\n",
        "plt.title(\"Q-Value Heatmap - Action-Value Pairs per Production State\", pad=20)\n",
        "plt.xlabel(\"Production State (P) with Action Pairs\", labelpad=15)\n",
        "plt.ylabel(\"Buffer (B) & Demand (D) States\", labelpad=15)\n",
        "heatmap.xaxis.tick_top()\n",
        "heatmap.xaxis.set_label_position('top')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k9efLt4VPcha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the trained Q-Learning agent"
      ],
      "metadata": {
        "id": "8Bsd5v64wXDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Evaluation of the trained Q-Learning agent\n",
        "#    with greedy action selection\n",
        "# -----------------------------------------------\n",
        "\n",
        "eval_episodes = 500\n",
        "eval_epsilon  = 0\n",
        "eval_rewards  = []\n",
        "\n",
        "def eps_greedy_policy(state_idx, Q, eps, act_space):\n",
        "    if np.random.rand() < eps:\n",
        "        return np.random.choice(act_space)\n",
        "    else:\n",
        "        return act_space[np.argmax(Q[state_idx])]\n",
        "\n",
        "act_space = np.arange(num_actions)\n",
        "\n",
        "for _ in range(eval_episodes):\n",
        "    state     = env.reset()\n",
        "    state_idx = state_to_index(state)\n",
        "    ep_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = eps_greedy_policy(state_idx, Q, eval_epsilon, act_space)\n",
        "        next_pt, next_bt, next_dt, reward, done = env.step(action)\n",
        "\n",
        "        state     = (next_pt, next_bt, next_dt)\n",
        "        state_idx = state_to_index(state)\n",
        "\n",
        "        ep_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    eval_rewards.append(ep_reward)\n",
        "\n",
        "print(f\"ε = {eval_epsilon}\")\n",
        "print(f\"Average reward from the QL trained agent (thesis) during {eval_episodes} episodes: {np.mean(eval_rewards):.2f}\")\n",
        "print(f\"Max episode reward: {np.max(eval_rewards):.2f}\")\n",
        "print(f\"Min episode reward:  {np.min(eval_rewards):.2f}\")\n"
      ],
      "metadata": {
        "id": "TNW78fWw_cD-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}